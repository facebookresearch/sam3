{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5673e897-0cb0-4be9-bdd8-740f7a6933e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# turn on tfloat32 for Ampere GPUs\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# use bfloat16 for the entire notebook. If your card doesn't support it, try float16 instead\n",
    "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c16b4-9c00-44ac-bf13-306420ef5610",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e03af1-6177-4085-ac38-e2622e2eab54",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f71794-5153-4212-845a-2b11b387b9f1",
   "metadata": {},
   "source": [
    "This section contains simple utilities to plot masks and bounding masks on top of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029788c1-12ee-426f-9500-0c9c4e5fc320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from matplotlib.colors import to_rgb\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def draw_box_on_image(image, box, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draws a rectangle on a given PIL image using the provided box coordinates in xywh format.\n",
    "    :param image: PIL.Image - The image on which to draw the rectangle.\n",
    "    :param box: tuple - A tuple (x, y, w, h) representing the top-left corner, width, and height of the rectangle.\n",
    "    :param color: tuple - A tuple (R, G, B) representing the color of the rectangle. Default is red.\n",
    "    :return: PIL.Image - The image with the rectangle drawn on it.\n",
    "    \"\"\"\n",
    "    # Ensure the image is in RGB mode\n",
    "    image = image.convert(\"RGB\")\n",
    "    # Unpack the box coordinates\n",
    "    x, y, w, h = box\n",
    "    x, y, w, h = int(x), int(y),int( w),int( h)\n",
    "    # Get the pixel data\n",
    "    pixels = image.load()\n",
    "    # Draw the top and bottom edges\n",
    "    for i in range(x, x + w):\n",
    "        pixels[i, y] = color\n",
    "        pixels[i, y + h - 1] = color\n",
    "        pixels[i, y+1] = color\n",
    "        pixels[i, y + h] = color\n",
    "        pixels[i, y-1] = color\n",
    "        pixels[i, y + h-2] = color\n",
    "    # Draw the left and right edges\n",
    "    for j in range(y, y + h):\n",
    "        pixels[x, j] = color\n",
    "        pixels[x+1, j] = color\n",
    "        pixels[x-1, j] = color\n",
    "        pixels[x + w - 1, j] = color\n",
    "        pixels[x + w, j] = color\n",
    "        pixels[x + w - 2, j] = color\n",
    "    return image\n",
    "\n",
    "\n",
    "def show_img_tensor(img_batch, vis_img_idx=0):\n",
    "    MEAN_IMG = np.array([0.485, 0.456, 0.406])\n",
    "    STD_IMG = np.array([0.229, 0.224, 0.225])\n",
    "    im_tensor = img_batch[vis_img_idx].detach().cpu()\n",
    "    assert im_tensor.dim() == 3\n",
    "    im_tensor = im_tensor.numpy().transpose((1, 2, 0))\n",
    "    im_tensor = (im_tensor * STD_IMG) + MEAN_IMG\n",
    "    im_tensor = np.clip(im_tensor, 0, 1)\n",
    "    plt.imshow(im_tensor)\n",
    "\n",
    "\n",
    "def show_points_with_labels(coords, labels, ax=None, marker_size=200):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color=\"green\", marker=\"*\", s=marker_size, edgecolor=\"white\", linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color=\"red\", marker=\"*\", s=marker_size, edgecolor=\"white\", linewidth=1.25)\n",
    "\n",
    "\n",
    "def plot_bbox(\n",
    "    img_height,\n",
    "    img_width,\n",
    "    box,\n",
    "    box_format=\"XYXY\",\n",
    "    relative_coords=True,\n",
    "    color=\"r\",\n",
    "    linestyle=\"solid\",\n",
    "    text=None,\n",
    "):\n",
    "    if box_format == \"XYXY\":\n",
    "        x, y, x2, y2 = box\n",
    "        w = x2 - x\n",
    "        h = y2 - y\n",
    "    elif box_format == \"XYWH\":\n",
    "        x, y, w, h = box\n",
    "    elif box_format == \"CxCyWH\":\n",
    "        cx, cy, w, h = box\n",
    "        x = cx - w / 2\n",
    "        y = cy - h / 2\n",
    "    else:\n",
    "        raise RuntimeError(f\"Invalid box_format {box_format}\")\n",
    "\n",
    "    if relative_coords:\n",
    "        x *= img_width\n",
    "        w *= img_width\n",
    "        y *= img_height\n",
    "        h *= img_height\n",
    "\n",
    "    rect = patches.Rectangle(\n",
    "        (x, y), w, h, linewidth=1.5, edgecolor=color, facecolor=\"none\", linestyle=linestyle,\n",
    "    )\n",
    "    plt.gca().add_patch(rect)\n",
    "\n",
    "    if text is not None:\n",
    "        facecolor = \"w\"\n",
    "        plt.gca().text(\n",
    "            x, y - 13, text, color=color, weight=\"bold\", fontsize=8,\n",
    "            bbox={\"facecolor\": facecolor, \"alpha\": 0.75, \"pad\": 2},\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_mask(mask, color=\"r\"):\n",
    "    im_h, im_w = mask.shape\n",
    "    mask_img = np.zeros((im_h, im_w, 4), dtype=np.float32)\n",
    "    mask_img[..., :3] = to_rgb(color)\n",
    "    mask_img[..., 3] = mask * 0.5\n",
    "    plt.imshow(mask_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f3853b-5573-43d8-9cd7-b7c4199015af",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75306c43-ba9d-4732-883d-076d00567578",
   "metadata": {},
   "source": [
    "This section contains some utility functions to create datapoints. They are optional, but give some good indication on how they should be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "968e5955-aab9-409e-9b78-1f6531a3dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3.train.data.sam3_image_dataset import InferenceMetadata, FindQueryLoaded, Image as SAMImage, Datapoint, QueryType\n",
    "GLOBAL_COUNTER = 1\n",
    "def create_empty_datapoint():\n",
    "    \"\"\" A datapoint is a single image on which we can apply several queries at once. \"\"\"\n",
    "    return Datapoint(find_queries=[], images=[])\n",
    "\n",
    "def set_image(datapoint, pil_image):\n",
    "    \"\"\" Add the image to be processed to the datapoint \"\"\"\n",
    "    w,h = pil_image.size\n",
    "    datapoint.images = [SAMImage(data=pil_image, objects=[], size=[h,w])]\n",
    "\n",
    "def add_text_prompt(datapoint, text_query):\n",
    "    \"\"\" Add a text query to the datapoint \"\"\"\n",
    "    \n",
    "    global GLOBAL_COUNTER\n",
    "    # in this function, we require that the image is already set.\n",
    "    # that's because we'll get its size to figure out what dimension to resize masks and boxes\n",
    "    # In practice you're free to set any size you want, just edit the rest of the function\n",
    "    assert len(datapoint.images) == 1, \"please set the image first\"\n",
    "\n",
    "    w, h = datapoint.images[0].size\n",
    "    datapoint.find_queries.append(\n",
    "        FindQueryLoaded(\n",
    "            query_type=QueryType.FindQuery,\n",
    "            query_text=text_query,\n",
    "            image_id=0,\n",
    "            object_ids_output=[], # unused for inference\n",
    "            is_exhaustive=True, # unused for inference\n",
    "            query_processing_order=0, \n",
    "            inference_metadata=InferenceMetadata(\n",
    "                coco_image_id=GLOBAL_COUNTER,\n",
    "                original_image_id=GLOBAL_COUNTER,\n",
    "                original_category_id=1,\n",
    "                original_size=[h,w],\n",
    "                object_id=0,\n",
    "                frame_index=0,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    GLOBAL_COUNTER += 1\n",
    "    return GLOBAL_COUNTER - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6b91a-7696-43b1-9ecf-b589ca163c3a",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66fedd11-da90-47c4-ba04-a2b55c5ecc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3 import build_sam3_image_model\n",
    "bpe_path = f\"/fsx-onevision/shared/dvc_cache_v3/files/md5/93/3b7abbbbde62c36f02f0e6ccde464f\"\n",
    "\n",
    "# checkpoint_path = f\"{sam3_root}/assets/checkpoints/sam3_prod_v12_interactive_5box_image_only.pt\"\n",
    "# has_presence_token = False\n",
    "\n",
    "checkpoint_path = f\"/fsx-onevision/shared/ckpts_fair_sc/checkpoint/sam3/shuangrui/omnivision_onevision/config/experiments/shuangrui/checkpoint_presence_0.5_completed.pt\"\n",
    "has_presence_token = True\n",
    "\n",
    "model = build_sam3_image_model(bpe_path=bpe_path, checkpoint_path=checkpoint_path).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fa4eed4-3928-47d6-8645-b7f5dad8c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3.train.transforms.basic_for_api import ComposeAPI, RandomResizeAPI, ToTensorAPI, NormalizeAPI\n",
    "transform = ComposeAPI(\n",
    "    transforms=[\n",
    "        RandomResizeAPI(sizes=1008, max_size=1009, square=True, consistent_transform=False),\n",
    "        ToTensorAPI(),\n",
    "        NormalizeAPI(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d471b-87b2-4fb1-a60b-f57a091ec996",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed594141-6837-4b23-97d4-141c6d1f5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image 1\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "img1 = Image.open(BytesIO(requests.get(\"http://images.cocodataset.org/val2017/000000077595.jpg\").content))\n",
    "datapoint1 = create_empty_datapoint()\n",
    "set_image(datapoint1, img1)\n",
    "id1 = add_text_prompt(datapoint1, \"cat\")\n",
    "id2 = add_text_prompt(datapoint1, \"laptop\")\n",
    "\n",
    "datapoint1 = transform(datapoint1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9ee9c1e-b879-4baa-b08e-3b592686ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = Image.open(BytesIO(requests.get(\"https://s3.us-east-1.amazonaws.com/images.cocodataset.org/val2017/000000136466.jpg\").content)) \n",
    "datapoint2 = create_empty_datapoint()\n",
    "set_image(datapoint2, img2)\n",
    "id3 = add_text_prompt(datapoint2, \"oven\")\n",
    "\n",
    "datapoint2 = transform(datapoint2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829416fb-b843-4d91-9792-7ca5e358ed68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52b9b87a-006f-4bc7-973f-59a041981ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3.train.data.collator import collate_fn_api as collate\n",
    "from sam3.model.data_misc import (\n",
    "    BatchedDatapoint,\n",
    "    BatchedPointer,\n",
    "    convert_my_tensors,\n",
    "    FindStage,\n",
    "    recursive_to,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59f8f39c-aed2-4d1b-b470-0c889329e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = collate([datapoint1, datapoint2], dict_key=\"dummy\")[\"dummy\"]\n",
    "batch = recursive_to(batch, torch.device(\"cuda\"), non_blocking=True)\n",
    "batch.img_batch = batch.img_batch.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d432e188-9132-44cc-a169-40d281d1fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/alcinos/miniconda/envs/sam3/lib/python3.12/site-packages/torch/_inductor/lowering.py:1917: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616ceb3-a507-408a-86ea-89a915a8337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39e1b9-9467-40af-a370-4f0f776a6651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
