{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs\n",
    "\n",
    "[x] Load SAM3 as a model\n",
    "[x] Gemini / OpenAI API support\n",
    "[ ] Final model checkpoint path\n",
    "[ ] clean up client_sam3.py\n",
    "[ ] dependencies\n",
    "[ ] Add a Short intro\n",
    "[ ] clean up TODOs in code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires 1 gpu to run SAM3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "output": {
     "id": 1340241197759010,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/home/jialez/code/sam3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jialez/miniconda3/envs/sam4_llama/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/home/jialez/miniconda3/envs/sam4_llama/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# Go to sam3 root dir\n",
    "%cd ~/code/sam3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled the use of perflib.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import to_rgb\n",
    "import pycocotools.mask as mask_utils\n",
    "\n",
    "\n",
    "from sam3 import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "from sam3.train.masks_ops import rle_encode\n",
    "from sam3.agent.helpers.mask_overlap_removal import remove_overlapping_masks\n",
    "from sam3.agent.viz import visualize\n",
    "from sam3.agent.agent_core import agent_inference\n",
    "from sam3.agent.client_llm import send_generate_request as send_generate_request_orig\n",
    "#from sam3.agent.client_sam3 import call_sam_service as call_sam_service_orig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_CONFIGS = {\n",
    "    # VLLM-served models\n",
    "    \"qwen2.5_7b\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    },\n",
    "    \"qwen2.5_72b\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
    "    },\n",
    "    \"qwen3_235b\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n",
    "    },\n",
    "    \"llama4_maverick\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    },\n",
    "    \"llama4_scout\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    },\n",
    "    \"gpt5\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"base_url\": \"https://api.openai.com/v1/\",\n",
    "        \"model\": \"gpt-5\",\n",
    "    },\n",
    "    \"gemini_pro\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"base_url\": \"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "        \"model\": \"gemini-2.5-pro\",\n",
    "    },\n",
    "    \"gemini_flash\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"base_url\": \"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "    }\n",
    "}\n",
    "\n",
    "model = \"qwen2.5_7b\" # @param [\"qwen2.5_7b,\"qwen2.5_72b\",\"llama4_maverick\",\"llama4_scout\",\"gpt5\",\"gemini_pro\",\"gemini_flash\"] {\"allow-input\":true}\n",
    "LLM_API_KEY = \"DUMMY_API_KEY\" # @param [\"DUMMY_API_KEY\"]\n",
    "\n",
    "#model = \"gpt5\"\n",
    "#LLM_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "#model = \"gemini_pro\"\n",
    "#LLM_API_KEY = \"YOUR_GEMINI_API_KEY\"\n",
    "\n",
    "\n",
    "llm_config = LLM_CONFIGS[model]\n",
    "llm_config[\"api_key\"] = LLM_API_KEY\n",
    "llm_config[\"name\"] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start LLM server, skip this step if you are calling LLM using an API\n",
    "\n",
    "# qwen 2.5VL 7B\n",
    "# vllm serve Qwen/Qwen2.5-VL-7B-Instruct --tensor-parallel-size 1 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "\n",
    "# qwen 2.5VL 72B\n",
    "#vllm serve Qwen/Qwen2.5-VL-72B-Instruct --tensor-parallel-size 8 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "\n",
    "# Llama 4 Maverick\n",
    "# VLLM_DISABLE_COMPILE_CACHE=1 vllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8   --tensor-parallel-size 8   --max-model-len 430000 --allowed-local-media-path / --port 8001\n",
    "\n",
    "# Llama 4 Scout\n",
    "# VLLM_DISABLE_COMPILE_CACHE=1 vllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8   --tensor-parallel-size 4   --max-model-len 30000 --allowed-local-media-path / --max-num-seqs 1 --port 8001\n",
    "\n",
    "if llm_config[\"provider\"] == \"vllm\":\n",
    "    #LLM_SERVER_URL = \"http://localhost:8001/v1\"\n",
    "    LLM_SERVER_URL = \"http://h200-017-014:8001/v1\" # TODO replace this with default LLM url\n",
    "else:\n",
    "    LLM_SERVER_URL = llm_config[\"base_url\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build SAM3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jialez/me/code\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the home directory\n",
    "home_dir = Path.home()\n",
    "# Construct the path to $HOME/me\n",
    "me_path = home_dir / \"me\" / \"code\"\n",
    "print(me_path)\n",
    "sam3_root = Path.home() / \"code\" / \"sam3\"\n",
    "bpe_path = f\"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam3_root = Path.home() / \"code\" / \"sam3\"\n",
    "\n",
    "bpe_path = f\"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz\"\n",
    "\n",
    "# checkpoint_path = f\"{sam3_root}/assets/checkpoints/sam3_prod_v12_interactive_5box_image_only.pt\"\n",
    "\n",
    "checkpoint_path = f\"{sam3_root}/assets/checkpoints/checkpoint.pt\"\n",
    "has_presence_token = True\n",
    "\n",
    "model = build_sam3_image_model(bpe_path=bpe_path, checkpoint_path=checkpoint_path, has_presence_token=has_presence_token)\n",
    "processor = Sam3Processor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam3_inference(model, processor, image_path, prompt):\n",
    "    image = Image.open(image_path)\n",
    "    inference_state = processor(image, instance_prompt=False)\n",
    "    #processor.reset_state(inference_state)\n",
    "    processor.add_prompt(inference_state, text_str=prompt,  instance_prompt=False)\n",
    "    model.run_inference(inference_state )\n",
    "    out = processor.postprocess_output(inference_state ,output_prob_thresh=0.5)\n",
    "\n",
    "    orig_img_w, orig_img_h = image.size\n",
    "\n",
    "    pred_masks = rle_encode(torch.from_numpy(out['out_binary_masks']))\n",
    "    pred_masks = [m['counts'] for m in pred_masks]\n",
    "    \n",
    "    outputs = {\n",
    "        \"orig_img_h\": orig_img_h,\n",
    "        \"orig_img_w\": orig_img_w,\n",
    "        \"pred_boxes\": out['out_boxes_xywh'].tolist(),\n",
    "        \"pred_masks\": pred_masks,\n",
    "        \"pred_scores\": out['out_probs'].tolist(),\n",
    "        #\"out_binary_masks\": out[\"out_binary_masks\"],\n",
    "    }\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_sam_service(image_path: str, text_prompt: str, output_folder_path: str = \"sam3_output\", threshold: float = 0.5, selected_masks: List[int]=None):\n",
    "    \"\"\"\n",
    "    Loads an image, sends it with a text prompt to the service,\n",
    "    saves the results, and renders the visualization.\n",
    "    \"\"\"\n",
    "    print(f\"üìû Loading image '{image_path}' and sending with prompt '{text_prompt}'...\")\n",
    "    \n",
    "    text_prompt_for_save_path = text_prompt.replace(\"/\", \"_\") if \"/\" in text_prompt else text_prompt\n",
    "    \n",
    "    os.makedirs(os.path.join(output_folder_path, image_path.replace(\"/\", \"-\")), exist_ok=True)\n",
    "    output_json_path = os.path.join(output_folder_path, image_path.replace(\"/\", \"-\"), rf\"{text_prompt_for_save_path}.json\")\n",
    "    output_image_path = os.path.join(output_folder_path, image_path.replace(\"/\", \"-\"), rf\"{text_prompt_for_save_path}.png\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send the image and text prompt as a multipart/form-data request\n",
    "        #with open(image_path, \"rb\") as f:\n",
    "        #    data = {'image_path': image_path, 'find_input_text': text_prompt, 'threshold': threshold}\n",
    "            #response = requests.post(server_url, data=data)\n",
    "    \n",
    "        #response.raise_for_status()\n",
    "        serialized_response = sam3_inference(model, processor, image_path, text_prompt)\n",
    "        \n",
    "        # 1. Get the raw JSON response from SAM3 Server\n",
    "        #serialized_response = response.json()\n",
    "        \n",
    "        # add remove duplicate masks\n",
    "        serialized_response = remove_overlapping_masks(serialized_response)\n",
    "        serialized_response = {\"original_image_path\": image_path, **serialized_response}\n",
    "        serialized_response = {\"output_image_path\": output_image_path, **serialized_response}\n",
    "        \n",
    "    \n",
    "        # 2. Reorder predictions by scores (highest to lowest) if scores are available\n",
    "        if 'pred_scores' in serialized_response and serialized_response['pred_scores']:\n",
    "            # Create indices sorted by scores in descending order\n",
    "            score_indices = sorted(range(len(serialized_response['pred_scores'])), \n",
    "                                 key=lambda i: serialized_response['pred_scores'][i], reverse=True)\n",
    "            \n",
    "            # Reorder all three lists based on the sorted indices\n",
    "            serialized_response['pred_scores'] = [serialized_response['pred_scores'][i] for i in score_indices]\n",
    "            serialized_response['pred_boxes'] = [serialized_response['pred_boxes'][i] for i in score_indices]\n",
    "            serialized_response['pred_masks'] = [serialized_response['pred_masks'][i] for i in score_indices]\n",
    "        \n",
    "        # 3. Remove any invalid RLE masks that is too short (shorter than 5 characters)\n",
    "        valid_masks = []\n",
    "        valid_boxes = []\n",
    "        valid_scores = []\n",
    "        for i, rle in enumerate(serialized_response['pred_masks']):\n",
    "            if len(rle) > 4:\n",
    "                valid_masks.append(rle)\n",
    "                valid_boxes.append(serialized_response['pred_boxes'][i])\n",
    "                valid_scores.append(serialized_response['pred_scores'][i])\n",
    "        serialized_response['pred_masks'] = valid_masks\n",
    "        serialized_response['pred_boxes'] = valid_boxes\n",
    "        serialized_response['pred_scores'] = valid_scores\n",
    "    \n",
    "        with open(output_json_path, 'w') as f:\n",
    "            json.dump(serialized_response, f, indent=4)\n",
    "        print(f\"‚úÖ Raw JSON response saved to '{output_json_path}'\")\n",
    "        \n",
    "        \n",
    "        # 4. Render and save visualizations on the image and save it in the SAM3 output folder\n",
    "        print(\"üîç Rendering visualizations on the image...\")\n",
    "        # pil_image = np.array(Image.open(image_path).convert('RGB'))\n",
    "        cv2_img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        boxes_array = np.array(serialized_response['pred_boxes'])\n",
    "        coco_rle_masks = [{'size': (serialized_response[\"orig_img_h\"], serialized_response[\"orig_img_w\"]), 'counts': rle} for rle in serialized_response['pred_masks']]\n",
    "        binary_masks = [mask_utils.decode(i) for i in coco_rle_masks]\n",
    "        viz_image = visualize(cv2_img, boxes_array, coco_rle_masks, binary_masks)\n",
    "        os.makedirs(os.path.dirname(output_image_path), exist_ok=True)\n",
    "        viz_image.save(output_image_path)\n",
    "        print(\"‚úÖ Saved visualization at:\", output_image_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error calling service: {e}\")\n",
    "    \n",
    "    return output_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_image_inference(image_path, text_prompt, llm_config, send_generate_request, call_sam_service, output_dir=\"agent_output\"):\n",
    "    \"\"\"Run inference on a single image with provided prompt\"\"\"\n",
    "\n",
    "    llm_name = llm_config[\"name\"]\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Generate output file names\n",
    "    image_basename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    prompt_for_filename = text_prompt.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "    base_filename = f\"{image_basename}_{prompt_for_filename}_Agent_{llm_name}\"\n",
    "    output_json_path = os.path.join(output_dir, f\"{base_filename}_Pred.json\")\n",
    "    output_image_path = os.path.join(output_dir, f\"{base_filename}_Pred.png\")\n",
    "    agent_history_path = os.path.join(output_dir, f\"{base_filename}_History.json\")\n",
    "\n",
    "    # Check if output already exists and skip\n",
    "    if os.path.exists(output_json_path):\n",
    "        print(f\"Output JSON {output_json_path} already exists. Skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n\\n\\n--------------Processing single image with prompt: {text_prompt}--------------\\n\")\n",
    "    print(f\"Image: {image_path}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    #try:\n",
    "    agent_history, final_output_dict, rendered_final_output = agent_inference(\n",
    "        image_path, text_prompt,\n",
    "        send_generate_request=send_generate_request,\n",
    "        call_sam_service=call_sam_service\n",
    "    )\n",
    "\n",
    "    final_output_dict[\"text_prompt\"] = text_prompt\n",
    "    final_output_dict[\"image_path\"] = image_path\n",
    "\n",
    "    # Save outputs\n",
    "    json.dump(final_output_dict, open(output_json_path, 'w'), indent=4)\n",
    "    json.dump(agent_history, open(agent_history_path, 'w'), indent=4)\n",
    "    rendered_final_output.save(output_image_path)\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully processed single image!\")\n",
    "    print(f\"Output JSON: {output_json_path}\")\n",
    "    print(f\"Output Image: {output_image_path}\")\n",
    "    print(f\"Agent History: {agent_history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SAM3 Agent Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "output": {
     "id": 689664053567678,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "--------------Processing single image with prompt: People wearing blue clothes--------------\n",
      "\n",
      "Image: /storage/home/jialez/code/sam3/assets/images/test_image.jpg\n",
      "Output directory: agent_output\n",
      "\n",
      "Initial text prompt:\n",
      "\n",
      " People wearing blue clothes\n",
      "\n",
      "\n",
      "Initial image path:\n",
      "\n",
      " /storage/home/jialez/code/sam3/assets/images/test_image.jpg\n",
      "image_path /storage/home/jialez/code/sam3/assets/images/test_image.jpg\n",
      "Calling model Qwen/Qwen2.5-VL-7B-Instruct...\n",
      "Received response: ChatCompletionMessage(content='<think>There is only one image in the message history (the raw input image). Since there is only one image, I will follow the Scenario 1 instructions: \\n1. Analyze: The image shows a group of children playing on a basketball court. Some of the children are wearing blue clothes, while others are wearing red and pink clothes.\\n2. Think: Based on the image, the target object(s) are the children wearing blue clothes. The initial user input query is straightforward and does not require any complex interpretation.\\n3. Remind: Each call to the segment_phrase tool will cause all previously generated mask(s) to be deleted (and can never be referred to again). So I should only call the segment_phrase tool on the whole primary grounding target(s), and not on a uniquely identifying part or attribute of the primary grounding target(s).\\n4. Plan: I will use the segment_phrase tool with the text_prompt \"people wearing blue clothes\" to generate mask(s) that cover the children wearing blue clothes.\\n5. Decide: The simple noun phrase \"people wearing blue clothes\" is suitable for calling the segment_phrase tool.\\n\\n</think>\\n<tool> {\"name\": \"segment_phrase\", \"parameters\": {\"text_prompt\": \"people wearing blue clothes\"}} </tool>', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None)\n",
      "\n",
      "\n",
      " Generated_text:\n",
      "\n",
      " <think>There is only one image in the message history (the raw input image). Since there is only one image, I will follow the Scenario 1 instructions: \n",
      "1. Analyze: The image shows a group of children playing on a basketball court. Some of the children are wearing blue clothes, while others are wearing red and pink clothes.\n",
      "2. Think: Based on the image, the target object(s) are the children wearing blue clothes. The initial user input query is straightforward and does not require any complex interpretation.\n",
      "3. Remind: Each call to the segment_phrase tool will cause all previously generated mask(s) to be deleted (and can never be referred to again). So I should only call the segment_phrase tool on the whole primary grounding target(s), and not on a uniquely identifying part or attribute of the primary grounding target(s).\n",
      "4. Plan: I will use the segment_phrase tool with the text_prompt \"people wearing blue clothes\" to generate mask(s) that cover the children wearing blue clothes.\n",
      "5. Decide: The simple noun phrase \"people wearing blue clothes\" is suitable for calling the segment_phrase tool.\n",
      "\n",
      "</think>\n",
      "<tool> {\"name\": \"segment_phrase\", \"parameters\": {\"text_prompt\": \"people wearing blue clothes\"}} </tool>\n",
      "üîç Calling segment_phrase tool...\n",
      "üìû Loading image '/storage/home/jialez/code/sam3/assets/images/test_image.jpg' and sending with prompt 'people wearing blue clothes'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jialez/miniconda3/envs/sam4_llama/lib/python3.10/site-packages/torch/nn/functional.py:5015: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/storage/home/jialez/code/sam3/sam3/model/decoder.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Raw JSON response saved to '/checkpoint/sam3/jialez//code/out/sam_out/-storage-home-jialez-code-sam3-assets-images-test_image.jpg/people wearing blue clothes.json'\n",
      "üîç Rendering visualizations on the image...\n",
      "‚úÖ Saved visualization at: /checkpoint/sam3/jialez//code/out/sam_out/-storage-home-jialez-code-sam3-assets-images-test_image.jpg/people wearing blue clothes.png\n",
      "\n",
      "\n",
      "sam3_output_text_message:\n",
      " The segment_phrase tool generated 3 available masks. All 3 available masks are rendered in this image below, now you must analyze the 3 available mask(s) carefully, compare them against the raw input image and the original user query, and determine your next action. Please be reminded that the original user query was 'People wearing blue clothes'.\n",
      "image_path /storage/home/jialez/code/sam3/assets/images/test_image.jpg\n",
      "image_path /checkpoint/sam3/jialez//code/out/sam_out/-storage-home-jialez-code-sam3-assets-images-test_image.jpg/people wearing blue clothes.png\n",
      "Calling model Qwen/Qwen2.5-VL-7B-Instruct...\n",
      "Received response: ChatCompletionMessage(content='<think>I have analyzed the 3 available masks generated by the segment_phrase tool. Mask 1 covers the child on the far right wearing a blue vest. Mask 2 covers the child in the middle wearing a blue vest. Mask 3 covers the child on the far left wearing a blue vest. These masks accurately represent the children wearing blue clothes as described in the user query. There are no other children in the image wearing blue clothes, so these are the only masks that match the user query.\\n\\n</think>\\n<tool> {\"name\": \"select_masks_and_return\", \"parameters\": {\"final_answer_masks\": [1, 2, 3]}} </tool>', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None)\n",
      "\n",
      "\n",
      " Generated_text:\n",
      "\n",
      " <think>I have analyzed the 3 available masks generated by the segment_phrase tool. Mask 1 covers the child on the far right wearing a blue vest. Mask 2 covers the child in the middle wearing a blue vest. Mask 3 covers the child on the far left wearing a blue vest. These masks accurately represent the children wearing blue clothes as described in the user query. There are no other children in the image wearing blue clothes, so these are the only masks that match the user query.\n",
      "\n",
      "</think>\n",
      "<tool> {\"name\": \"select_masks_and_return\", \"parameters\": {\"final_answer_masks\": [1, 2, 3]}} </tool>\n",
      "üîç Calling select_masks_and_return tool...\n",
      "\n",
      "‚úÖ Successfully processed single image!\n",
      "Output JSON: agent_output/test_image_People_wearing_blue_clothes_Agent_qwen2.5_7b_Pred.json\n",
      "Output Image: agent_output/test_image_People_wearing_blue_clothes_Agent_qwen2.5_7b_Pred.png\n",
      "Agent History: agent_output/test_image_People_wearing_blue_clothes_Agent_qwen2.5_7b_History.json\n"
     ]
    }
   ],
   "source": [
    "image = \"assets/images/test_image.jpg\"\n",
    "prompt = \"People wearing blue clothes\"\n",
    "\n",
    "# get absolute path for image\n",
    "image = os.path.abspath(image)\n",
    "send_generate_request = partial(send_generate_request_orig, server_url=LLM_SERVER_URL, model=llm_config[\"model\"], api_key=llm_config[\"api_key\"])\n",
    "#call_sam_service = partial(call_sam_service_orig, server_url=SAM3_SERVICE_URL)\n",
    "\n",
    "# Run single image inference\n",
    "run_single_image_inference(image, prompt, llm_config, send_generate_request, call_sam_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "be59e249-6c09-4634-a9e7-1f06fd233c42",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
