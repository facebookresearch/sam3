{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM 3 Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires a single gpu to run. Tested on Nvidia H100 with 80GB memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SAM3_ROOT = str(Path.cwd().parent)\n",
    "# Go to sam3 root dir\n",
    "os.chdir(SAM3_ROOT)\n",
    "\n",
    "# setup GPU to use -  A single GPU is good with the purpose of this demo\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "_ = os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import to_rgb\n",
    "import pycocotools.mask as mask_utils\n",
    "\n",
    "\n",
    "from sam3 import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "from sam3.train.masks_ops import rle_encode\n",
    "from sam3.agent.helpers.mask_overlap_removal import remove_overlapping_masks\n",
    "from sam3.agent.viz import visualize\n",
    "from sam3.agent.agent_core import agent_inference\n",
    "from sam3.agent.client_llm import send_generate_request as send_generate_request_orig\n",
    "from sam3.model.box_ops import box_xyxy_to_xywh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_CONFIGS = {\n",
    "    # VLLM-served models\n",
    "    \"qwen2.5_7b\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    },\n",
    "    \"qwen2.5_72b\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
    "    },\n",
    "    \"qwen3_235b\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n",
    "    },\n",
    "    \"llama4_maverick\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    },\n",
    "    \"llama4_scout\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    },\n",
    "    \"gpt5\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"base_url\": \"https://api.openai.com/v1/\",\n",
    "        \"model\": \"gpt-5\",\n",
    "    },\n",
    "    \"gemini_pro\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"base_url\": \"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "        \"model\": \"gemini-2.5-pro\",\n",
    "    },\n",
    "    \"gemini_flash\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"base_url\": \"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "    }\n",
    "}\n",
    "\n",
    "model = \"qwen2.5_7b\" # @param [\"qwen2.5_7b,\"qwen2.5_72b\",\"llama4_maverick\",\"llama4_scout\",\"gpt5\",\"gemini_pro\",\"gemini_flash\"] {\"allow-input\":true}\n",
    "LLM_API_KEY = \"DUMMY_API_KEY\" # @param [\"DUMMY_API_KEY\"]\n",
    "\n",
    "#model = \"gpt5\"\n",
    "#LLM_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "#model = \"gemini_pro\"\n",
    "#LLM_API_KEY = \"YOUR_GEMINI_API_KEY\"\n",
    "\n",
    "\n",
    "llm_config = LLM_CONFIGS[model]\n",
    "llm_config[\"api_key\"] = LLM_API_KEY\n",
    "llm_config[\"name\"] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup vLLM server (optional)\n",
    "This step is only required if you are using a model served by vLLM, skip this step if you are calling LLM using an API.\n",
    "\n",
    "* Install vLLM\n",
    "  ```bash\n",
    "    conda create -n vllm python=3.12\n",
    "    pip install vllm==0.10.1\n",
    "  ```\n",
    "* Start vLLM server\n",
    "  ```bash\n",
    "    # qwen 2.5VL 7B\n",
    "    vllm serve Qwen/Qwen2.5-VL-7B-Instruct --tensor-parallel-size 1 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "    \n",
    "    # qwen 2.5VL 72B\n",
    "    vllm serve Qwen/Qwen2.5-VL-72B-Instruct --tensor-parallel-size 8 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "    \n",
    "    # Llama 4 Maverick\n",
    "    VLLM_DISABLE_COMPILE_CACHE=1 vllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8   --tensor-parallel-size 8   --max-model-len 430000 --allowed-local-media-path / --port 8001\n",
    "    \n",
    "    # Llama 4 Scout\n",
    "    VLLM_DISABLE_COMPILE_CACHE=1 vllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8   --tensor-parallel-size 4   --max-model-len 30000 --allowed-local-media-path / --max-num-seqs 1 --port 8001\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_config[\"provider\"] == \"vllm\":\n",
    "    LLM_SERVER_URL = \"http://0.0.0.0:8001/v1\"  # replace this with your vLLM server address as needed\n",
    "else:\n",
    "    LLM_SERVER_URL = llm_config[\"base_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build SAM3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = os.path.join(SAM3_ROOT, \"assets/bpe_simple_vocab_16e6.txt.gz\")\n",
    "# checkpoint_path = \"/home/jialez/code/sam3/assets/checkpoints/sam3_v2_rc2_fair_sc.pt\"\n",
    "checkpoint_path = \"/checkpoint/sam3/chayryali/omnivision_onevision/config/experiments/chayryali/sam3v1_stage3/final/paper_launch_model_oct/launch_stage3_v2.yaml/0/checkpoints/checkpoint.pt\"\n",
    "# checkpoint_path = \"/home/jialez/code/sam3/assets/checkpoints/checkpoint.pt\"\n",
    "has_presence_token = True\n",
    "\n",
    "model = build_sam3_image_model(\n",
    "    bpe_path=bpe_path, \n",
    "    checkpoint_path=checkpoint_path\n",
    ").cuda()\n",
    "processor = Sam3Processor(model, confidence_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam3_inference(processor, image_path, text_prompt):\n",
    "    image = Image.open(image_path)\n",
    "    orig_img_w, orig_img_h = image.size\n",
    "    inference_state = processor.set_image(image)\n",
    "    inference_state= processor.set_text_prompt(state=inference_state, prompt=text_prompt)\n",
    "    pred_boxes_xyxy = torch.stack([\n",
    "        inference_state[\"boxes\"][:, 0] / orig_img_w,\n",
    "        inference_state[\"boxes\"][:, 1] / orig_img_h,\n",
    "        inference_state[\"boxes\"][:, 2] / orig_img_w,\n",
    "        inference_state[\"boxes\"][:, 3] / orig_img_h,        \n",
    "    ], dim=-1)  # normalized in range [0, 1]\n",
    "    pred_boxes_xywh = box_xyxy_to_xywh(pred_boxes_xyxy).tolist()\n",
    "    pred_masks = rle_encode(inference_state[\"masks\"].squeeze(1))\n",
    "    pred_masks = [m['counts'] for m in pred_masks]\n",
    "    \n",
    "    outputs = {\n",
    "        \"orig_img_h\": orig_img_h,\n",
    "        \"orig_img_w\": orig_img_w,\n",
    "        \"pred_boxes\": pred_boxes_xywh,\n",
    "        \"pred_masks\": pred_masks,\n",
    "        \"pred_scores\": inference_state[\"scores\"].tolist(),\n",
    "    }\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_sam_service(image_path: str, text_prompt: str, output_folder_path: str = \"sam3_output\", threshold: float = 0.5, selected_masks: List[int]=None):\n",
    "    \"\"\"\n",
    "    Loads an image, sends it with a text prompt to the service,\n",
    "    saves the results, and renders the visualization.\n",
    "    \"\"\"\n",
    "    print(f\"üìû Loading image '{image_path}' and sending with prompt '{text_prompt}'...\")\n",
    "    \n",
    "    text_prompt_for_save_path = text_prompt.replace(\"/\", \"_\") if \"/\" in text_prompt else text_prompt\n",
    "    \n",
    "    os.makedirs(os.path.join(output_folder_path, image_path.replace(\"/\", \"-\")), exist_ok=True)\n",
    "    output_json_path = os.path.join(output_folder_path, image_path.replace(\"/\", \"-\"), rf\"{text_prompt_for_save_path}.json\")\n",
    "    output_image_path = os.path.join(output_folder_path, image_path.replace(\"/\", \"-\"), rf\"{text_prompt_for_save_path}.png\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send the image and text prompt as a multipart/form-data request\n",
    "        serialized_response = sam3_inference(processor, image_path, text_prompt)\n",
    "        \n",
    "        # 1. Get the raw JSON response from SAM3 Server\n",
    "        #serialized_response = response.json()\n",
    "        \n",
    "        # add remove duplicate masks\n",
    "        serialized_response = remove_overlapping_masks(serialized_response)\n",
    "        serialized_response = {\"original_image_path\": image_path, **serialized_response}\n",
    "        serialized_response = {\"output_image_path\": output_image_path, **serialized_response}\n",
    "        \n",
    "    \n",
    "        # 2. Reorder predictions by scores (highest to lowest) if scores are available\n",
    "        if 'pred_scores' in serialized_response and serialized_response['pred_scores']:\n",
    "            # Create indices sorted by scores in descending order\n",
    "            score_indices = sorted(range(len(serialized_response['pred_scores'])), \n",
    "                                 key=lambda i: serialized_response['pred_scores'][i], reverse=True)\n",
    "            \n",
    "            # Reorder all three lists based on the sorted indices\n",
    "            serialized_response['pred_scores'] = [serialized_response['pred_scores'][i] for i in score_indices]\n",
    "            serialized_response['pred_boxes'] = [serialized_response['pred_boxes'][i] for i in score_indices]\n",
    "            serialized_response['pred_masks'] = [serialized_response['pred_masks'][i] for i in score_indices]\n",
    "        \n",
    "        # 3. Remove any invalid RLE masks that is too short (shorter than 5 characters)\n",
    "        valid_masks = []\n",
    "        valid_boxes = []\n",
    "        valid_scores = []\n",
    "        for i, rle in enumerate(serialized_response['pred_masks']):\n",
    "            if len(rle) > 4:\n",
    "                valid_masks.append(rle)\n",
    "                valid_boxes.append(serialized_response['pred_boxes'][i])\n",
    "                valid_scores.append(serialized_response['pred_scores'][i])\n",
    "        serialized_response['pred_masks'] = valid_masks\n",
    "        serialized_response['pred_boxes'] = valid_boxes\n",
    "        serialized_response['pred_scores'] = valid_scores\n",
    "    \n",
    "        with open(output_json_path, 'w') as f:\n",
    "            json.dump(serialized_response, f, indent=4)\n",
    "        print(f\"‚úÖ Raw JSON response saved to '{output_json_path}'\")\n",
    "        \n",
    "        \n",
    "        # 4. Render and save visualizations on the image and save it in the SAM3 output folder\n",
    "        print(\"üîç Rendering visualizations on the image...\")\n",
    "        # pil_image = np.array(Image.open(image_path).convert('RGB'))\n",
    "        cv2_img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        boxes_array = np.array(serialized_response['pred_boxes'])\n",
    "        coco_rle_masks = [{'size': (serialized_response[\"orig_img_h\"], serialized_response[\"orig_img_w\"]), 'counts': rle} for rle in serialized_response['pred_masks']]\n",
    "        binary_masks = [mask_utils.decode(i) for i in coco_rle_masks]\n",
    "        viz_image = visualize(serialized_response, boxes_array, coco_rle_masks, binary_masks)\n",
    "        print(f\"call_sam_service output_image_path = {output_image_path}\")\n",
    "        os.makedirs(os.path.dirname(output_image_path), exist_ok=True)\n",
    "        viz_image.save(output_image_path)\n",
    "        print(\"‚úÖ Saved visualization at:\", output_image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error calling service: {e}\")\n",
    "    \n",
    "    return output_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_image_inference(image_path, text_prompt, llm_config, send_generate_request, call_sam_service, output_dir=\"agent_output\"):\n",
    "    \"\"\"Run inference on a single image with provided prompt\"\"\"\n",
    "\n",
    "    llm_name = llm_config[\"name\"]\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Generate output file names\n",
    "    image_basename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    prompt_for_filename = text_prompt.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "    base_filename = f\"{image_basename}_{prompt_for_filename}_Agent_{llm_name}\"\n",
    "    output_json_path = os.path.join(output_dir, f\"{base_filename}_Pred.json\")\n",
    "    output_image_path = os.path.join(output_dir, f\"{base_filename}_Pred.png\")\n",
    "    agent_history_path = os.path.join(output_dir, f\"{base_filename}_History.json\")\n",
    "\n",
    "    # Check if output already exists and skip\n",
    "    if os.path.exists(output_json_path):\n",
    "        print(f\"Output JSON {output_json_path} already exists. Skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n\\n\\n--------------Processing single image with prompt: {text_prompt}--------------\\n\")\n",
    "    print(f\"Image: {image_path}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    #try:\n",
    "    agent_history, final_output_dict, rendered_final_output = agent_inference(\n",
    "        image_path, text_prompt,\n",
    "        send_generate_request=send_generate_request,\n",
    "        call_sam_service=call_sam_service,\n",
    "        output_dir=output_dir,\n",
    "    )\n",
    "\n",
    "    final_output_dict[\"text_prompt\"] = text_prompt\n",
    "    final_output_dict[\"image_path\"] = image_path\n",
    "\n",
    "    # Save outputs\n",
    "    json.dump(final_output_dict, open(output_json_path, 'w'), indent=4)\n",
    "    json.dump(agent_history, open(agent_history_path, 'w'), indent=4)\n",
    "    rendered_final_output.save(output_image_path)\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully processed single image!\")\n",
    "    print(f\"Output JSON: {output_json_path}\")\n",
    "    print(f\"Output Image: {output_image_path}\")\n",
    "    print(f\"Agent History: {agent_history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SAM3 Agent Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 689664053567678,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "image = \"assets/images/test_image.jpg\"\n",
    "prompt = \"People wearing blue clothes\"\n",
    "\n",
    "# get absolute path for image\n",
    "image = os.path.abspath(image)\n",
    "send_generate_request = partial(send_generate_request_orig, server_url=LLM_SERVER_URL, model=llm_config[\"model\"], api_key=llm_config[\"api_key\"])\n",
    "#call_sam_service = partial(call_sam_service_orig, server_url=SAM3_SERVICE_URL)\n",
    "# Run single image inference\n",
    "run_single_image_inference(image, prompt, llm_config, send_generate_request, call_sam_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "be59e249-6c09-4634-a9e7-1f06fd233c42",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
