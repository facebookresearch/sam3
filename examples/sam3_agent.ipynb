{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM 3 Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows an example of how an MLLM can use SAM 3 as a tool, i.e., \"SAM 3 Agent\", to segment more complex text queries such as \"the rightmost person wearing blue vest\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install `sam3` in your environment using the [installation instructions](https://github.com/facebookresearch/sam3?tab=readme-ov-file#installation) in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# turn on tfloat32 for Ampere GPUs\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# use bfloat16 for the entire notebook. If your card doesn't support it, try float16 instead\n",
    "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "# inference mode for the whole notebook. Disable if you need gradients\n",
    "torch.inference_mode().__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 31 00:15:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:53:00.0 Off |                    0 |\n",
      "| N/A   26C    P0             69W /  700W |       4MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             68W /  700W |       4MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SAM3_ROOT = os.path.dirname(os.getcwd())\n",
    "os.chdir(SAM3_ROOT)\n",
    "\n",
    "# setup GPU to use -  A single GPU is good with the purpose of this demo\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "_ = os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SAM3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3 import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "\n",
    "bpe_path = os.path.join(SAM3_ROOT, \"assets/bpe_simple_vocab_16e6.txt.gz\")\n",
    "checkpoint_path = \"/checkpoint/sam3/chayryali/omnivision_onevision/config/experiments/chayryali/sam3v1_stage3/final/paper_launch_model_oct/launch_stage3_v2.yaml/0/checkpoints/checkpoint.pt\"\n",
    "\n",
    "model = build_sam3_image_model(bpe_path=bpe_path, checkpoint_path=checkpoint_path).cuda()\n",
    "processor = Sam3Processor(model, confidence_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup\n",
    "\n",
    "Config which MLLM to use, it can either be a model served by vLLM that you launch from your own machine or a model is served via external API. If you want to using a vLLM model, we also provided insturctions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_CONFIGS = {\n",
    "    # vLLM-served models\n",
    "    \"qwen2.5_7b\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    },\n",
    "    \"qwen2.5_72b\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
    "    },\n",
    "    \"qwen3_vl_2b_instruct\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen3-VL-2B-Instruct\",\n",
    "    },\n",
    "    \"qwen3_vl_2b_thinking\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen3-VL-2B-Thinking\",\n",
    "    },\n",
    "    \"qwen3_vl_4b_instruct\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen3-VL-4B-Instruct\",\n",
    "    },\n",
    "    \"qwen3_vl_4b_thinking\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen3-VL-4B-Thinking\",\n",
    "    },\n",
    "    \"qwen3_vl_8b_instruct\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen3-VL-8B-Instruct\",\n",
    "    },\n",
    "    \"qwen3_vl_8b_thinking\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"Qwen/Qwen3-VL-8B-Thinking\",\n",
    "    }, \n",
    "    \"llama4_maverick\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    },\n",
    "    \"llama4_scout\": {\n",
    "        \"provider\": \"vllm\",\n",
    "        \"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    },\n",
    "    # models with external API calls\n",
    "    \"gpt5\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"base_url\": \"https://api.openai.com/v1/\",\n",
    "        \"model\": \"gpt-5\",\n",
    "    },\n",
    "    \"gemini_pro\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"base_url\": \"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "        \"model\": \"gemini-2.5-pro\",\n",
    "    },\n",
    "    \"gemini_flash\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"base_url\": \"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "    }\n",
    "}\n",
    "\n",
    "model = \"qwen3_vl_8b_thinking\"\n",
    "# model = \"qwen2.5_7b\"\n",
    "LLM_API_KEY = \"DUMMY_API_KEY\"\n",
    "\n",
    "# model = \"gpt5\"\n",
    "# LLM_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# model = \"gemini_pro\"\n",
    "# LLM_API_KEY = \"YOUR_GEMINI_API_KEY\"\n",
    "\n",
    "llm_config = LLM_CONFIGS[model]\n",
    "llm_config[\"api_key\"] = LLM_API_KEY\n",
    "llm_config[\"name\"] = model\n",
    "\n",
    "# setup API endpoint\n",
    "if llm_config[\"provider\"] == \"vllm\":\n",
    "    LLM_SERVER_URL = \"http://0.0.0.0:8001/v1\"  # replace this with your vLLM server address as needed\n",
    "else:\n",
    "    LLM_SERVER_URL = llm_config[\"base_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup vLLM server (optional)\n",
    "This step is only required if you are using a model served by vLLM, skip this step if you are calling LLM using an API like Gemini and GPT.\n",
    "\n",
    "* Install vLLM (in a separate conda env from SAM 3 to avoid dependency conflicts).\n",
    "  ```bash\n",
    "    conda create -n vllm python=3.12\n",
    "    pip install vllm --extra-index-url https://download.pytorch.org/whl/cu128\n",
    "  ```\n",
    "* Start vLLM server on the same machine of this notebook\n",
    "  ```bash\n",
    "    # qwen 2.5 VL 7B\n",
    "    vllm serve Qwen/Qwen2.5-VL-7B-Instruct --tensor-parallel-size 1 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "    \n",
    "    # qwen 2.5 VL 72B\n",
    "    vllm serve Qwen/Qwen2.5-VL-72B-Instruct --tensor-parallel-size 8 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "\n",
    "    # qwen 3 VL 2B instruct\n",
    "    vllm serve Qwen/Qwen3-VL-2B-Instruct --tensor-parallel-size 1 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "\n",
    "    # qwen 3 VL 2B thinking\n",
    "    vllm serve Qwen/Qwen3-VL-2B-Thinking --tensor-parallel-size 1 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "\n",
    "    # qwen 3 VL 4B instruct\n",
    "    vllm serve Qwen/Qwen3-VL-4B-Instruct --tensor-parallel-size 1 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "\n",
    "    # qwen 3 VL 4B thinking\n",
    "    vllm serve Qwen/Qwen3-VL-4B-Thinking --tensor-parallel-size 1 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "\n",
    "    # qwen 3 VL 8B instruct\n",
    "    vllm serve Qwen/Qwen3-VL-8B-Instruct --tensor-parallel-size 1 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "\n",
    "    # qwen 3 VL 8B thinking\n",
    "    vllm serve Qwen/Qwen3-VL-8B-Thinking --tensor-parallel-size 1 --allowed-local-media-path / --enforce-eager --port 8001\n",
    "\n",
    "    # Llama 4 Maverick\n",
    "    VLLM_DISABLE_COMPILE_CACHE=1 vllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8   --tensor-parallel-size 8   --max-model-len 430000 --allowed-local-media-path / --port 8001\n",
    "    \n",
    "    # Llama 4 Scout\n",
    "    VLLM_DISABLE_COMPILE_CACHE=1 vllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8   --tensor-parallel-size 4   --max-model-len 30000 --allowed-local-media-path / --max-num-seqs 1 --port 8001\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SAM3 Agent Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/jielei/sam3/sam3/agent/agent_core.py:203: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from IPython.display import display, Image\n",
    "from sam3.agent.client_llm import send_generate_request as send_generate_request_orig\n",
    "from sam3.agent.client_sam3 import call_sam_service as call_sam_service_orig\n",
    "from sam3.agent.inference import run_single_image_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 689664053567678,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Starting SAM 3 Agent Session... ------------------------------ \n",
      "> Text prompt: the rightmost person wearing blue vest\n",
      "> Image path: /storage/home/jielei/sam3/assets/images/test_image.jpg\n",
      "\n",
      "\n",
      "\n",
      "------------------------------ Round 1------------------------------\n",
      "\n",
      "\n",
      "\n",
      "image_path /storage/home/jielei/sam3/assets/images/test_image.jpg\n",
      "üîç Calling model Qwen/Qwen3-VL-8B-Thinking...\n",
      "\n",
      ">>> MLLM Response [start]\n",
      "State that there is only one image in the message history (the raw input image). Since there is only one image, you will follow the Scenario 1 instructions: \n",
      "1. Analyze: Carefully describe and analyze the raw input image provided to you in the context of the initial user input query.\n",
      "2. Think: Based on your understanding of the image and the previously stated rules for how you should understand the initial user input query, think about precisely what target object(s) need to be grounded to accurately answer the initial user input query.\n",
      "3. Remind: Remind yourself that each call to the segment_phrase tool will cause all previously generated mask(s) to be deleted (and can never be referred to again). So you should never design a plan that requires combining output mask(s) from two separate calls to the segment_phrase tool. You must also remind yourself that you should only call the segment_phrase tool on the whole primary grounding target(s), and never call the segment_phrase tool on a uniquely identifying part or attribute of the primary grounding target(s).\n",
      "4. Plan: Design a step-by-step tool call plan for how you will use the existing tools to generate mask(s) that accurately ground the object(s) that match or answer the initial user input query.\n",
      "5. Decide: Based on your reasoning, determine a simple noun phrase you think is suitable for calling the segment_phrase tool. The phrase should be a simple, direct, singular noun phrase. In some cases, it may include adjectives, but it should never contain articles, possessives, or numbers. \n",
      "</think>\n",
      "\n",
      "1. **Analyze**: The image shows six children running on a basketball court. The background includes a basketball hoop, a building with a green \"EXIT\" sign, and a wooden fence. The children are wearing either red or blue vests, with some in black pants and others in gray pants. The rightmost child is wearing a blue vest and white shoes, positioned on the far right side of the image.\n",
      "\n",
      "2. **Think**: The user‚Äôs query asks for \"the rightmost person wearing blue vest\". This is a specific instance of a person, not a part of a person (e.g., not just the vest). There are multiple children in blue vests, but only one is the rightmost. The target object is the person (entire body), not the vest or any other attribute.\n",
      "\n",
      "3. **Remind**: Each segment_phrase call deletes prior masks, so I must ground the entire person, not a part. I should avoid phrases like \"blue vest\" or \"rightmost child\" as they reference sub-parts or positions.\n",
      "\n",
      "4. **Plan**: Use segment_phrase on \"person\" to generate all individuals, then narrow down to the rightmost blue vest wearer using the select_masks_and_return tool. Alternatively, use \"blue vest person\" to directly target the relevant individuals.\n",
      "\n",
      "5. **Decide**: The best noun phrase for segment_phrase is \"blue vest person\", as it directly refers to the entire person wearing a blue vest, which matches the query. This phrase avoids referencing parts or attributes unnecessarily and will generate the correct masks for all blue vest individuals, allowing me to select the rightmost one in the next step.\n",
      "\n",
      "<tool> {\"name\": \"segment_phrase\", \"parameters\": {\"text_prompt\": \"blue vest person\"}} </tool>\n",
      "<<< MLLM Response [end]\n",
      "\n",
      "üîç Calling segment_phrase tool...\n",
      "üìû Loading image '/storage/home/jielei/sam3/assets/images/test_image.jpg' and sending with prompt 'blue vest person'...\n",
      "‚úÖ Raw JSON response saved to 'agent_output/sam_out/-storage-home-jielei-sam3-assets-images-test_image.jpg/blue vest person.json'\n",
      "üîç Rendering visualizations on the image ...\n",
      "‚úÖ Saved visualization at: agent_output/sam_out/-storage-home-jielei-sam3-assets-images-test_image.jpg/blue vest person.png\n",
      "‚ùå No masks generated by SAM3, reporting no mask to Qwen.\n",
      "\n",
      "\n",
      ">>> sam3_output_text_message:\n",
      " The segment_phrase tool did not generate any masks for the text_prompt 'blue vest person'. Now, please call the segment_phrase tool again with a different, perhaps more general, or more creative simple noun phrase text_prompt, while adhering to all the rules stated in the system prompt. Please be reminded that the original user query was 'the rightmost person wearing blue vest'.\n",
      "\n",
      "\n",
      "\n",
      "------------------------------ Round 2------------------------------\n",
      "\n",
      "\n",
      "\n",
      "image_path /storage/home/jielei/sam3/assets/images/test_image.jpg\n",
      "üîç Calling model Qwen/Qwen3-VL-8B-Thinking...\n",
      "\n",
      ">>> MLLM Response [start]\n",
      "State that there is only one image in the message history (the raw input image). Since there is only one image, you will follow the Scenario 1 instructions: \n",
      "1. Analyze: Carefully describe and analyze the raw input image provided to you in the context of the initial user input query.\n",
      "2. Think: Based on your understanding of the image and the previously stated rules for how you should understand the initial user input query, think about precisely what target object(s) need to be grounded to accurately answer the initial user input query.\n",
      "3. Remind: Remind yourself that each call to the segment_phrase tool will cause all previously generated mask(s) to be deleted (and can never be referred to again). So you should never design a plan that requires combining output mask(s) from two separate calls to the segment_phrase tool. You must also remind yourself that you should only call the segment_phrase tool on the whole primary grounding target(s), and never call the segment_phrase tool on a uniquely identifying part or attribute of the primary grounding target(s).\n",
      "4. Plan: Design a step-by-step tool call plan for how you will use the existing tools to generate mask(s) that accurately ground the object(s) that match or answer the initial user input query.\n",
      "5. Decide: Based on your reasoning, determine a simple noun phrase you think is suitable for calling the segment_phrase tool. The phrase should be a simple, direct, singular noun phrase. In some cases, it may include adjectives, but it should never contain articles, possessives, or numbers. <think>\n",
      "The segment_phrase tool failed with \"blue vest person\", likely because the model doesn't recognize \"vest\" as a key term for segmentation. I should try a more general term like \"person\" first, then use select_masks_and_return to filter by vest color and position. Since the user specified \"rightmost\", I'll check if \"person\" generates all masks, then filter for the rightmost individual with blue vest.\n",
      "</think>\n",
      "<tool> {\"name\": \"segment_phrase\", \"parameters\": {\"text_prompt\": \"person\"}} </tool>\n",
      "<<< MLLM Response [end]\n",
      "\n",
      "üîç Calling segment_phrase tool...\n",
      "üìû Loading image '/storage/home/jielei/sam3/assets/images/test_image.jpg' and sending with prompt 'person'...\n",
      "‚úÖ Raw JSON response saved to 'agent_output/sam_out/-storage-home-jielei-sam3-assets-images-test_image.jpg/person.json'\n",
      "üîç Rendering visualizations on the image ...\n",
      "‚úÖ Saved visualization at: agent_output/sam_out/-storage-home-jielei-sam3-assets-images-test_image.jpg/person.png\n",
      "\n",
      "\n",
      ">>> sam3_output_text_message:\n",
      " The segment_phrase tool generated 6 available masks. All 6 available masks are rendered in this image below, now you must analyze the 6 available mask(s) carefully, compare them against the raw input image and the original user query, and determine your next action. Please be reminded that the original user query was 'the rightmost person wearing blue vest'.\n",
      "\n",
      "\n",
      "\n",
      "------------------------------ Round 3------------------------------\n",
      "\n",
      "\n",
      "\n",
      "image_path /storage/home/jielei/sam3/assets/images/test_image.jpg\n",
      "image_path agent_output/sam_out/-storage-home-jielei-sam3-assets-images-test_image.jpg/person.png\n",
      "üîç Calling model Qwen/Qwen3-VL-8B-Thinking...\n"
     ]
    }
   ],
   "source": [
    "# prepare input args and run single image inference\n",
    "image = \"assets/images/test_image.jpg\"\n",
    "prompt = \"the rightmost person wearing blue vest\"\n",
    "image = os.path.abspath(image)\n",
    "send_generate_request = partial(send_generate_request_orig, server_url=LLM_SERVER_URL, model=llm_config[\"model\"], api_key=llm_config[\"api_key\"])\n",
    "call_sam_service = partial(call_sam_service_orig, sam3_processor=processor)\n",
    "output_image_path = run_single_image_inference(image, prompt, llm_config, send_generate_request, call_sam_service, debug=True)\n",
    "\n",
    "# display output\n",
    "if output_image_path is not None:\n",
    "    display(Image(filename=output_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "be59e249-6c09-4634-a9e7-1f06fd233c42",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
