{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video segmentation and tracking with SAM 3\n",
    "\n",
    "### This notebook demonstrates how to use SAM 3 for interactive video segmentation and dense tracking. It covers the following capabilities:\n",
    "\n",
    "- **Text prompts**: Using natural language descriptions to segment objects (e.g., \"person\", \"face\", \"visual\")\n",
    "- **Point prompts**: Adding positive/negative clicks to segment and refine objects\n",
    "- **Box prompts**: Using bounding boxes combined with text for precise object localization\n",
    "- **Interactive refinement**: Adding clicks on any frame to improve segmentation quality\n",
    "\n",
    "#### We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video. \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam3/blob/main/notebooks/sam3_video_predictor_example.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib scikit-learn\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam3.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# use all available GPUs on the machine\n",
    "gpus_to_use = range(torch.cuda.device_count())\n",
    "\n",
    "# # use only a single GPU\n",
    "# gpus_to_use = [torch.cuda.current_device()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sam3.visualization_utils import (\n",
    "    visualize_formatted_frame_output,\n",
    "    prepare_masks_for_visualization,\n",
    "    visualize_prompt_overlay,\n",
    "    show_points,\n",
    "    draw_box_on_image,\n",
    "    load_frame,\n",
    ")\n",
    "\n",
    "# helper functions\n",
    "def propagate_in_video(predictor, session_id):\n",
    "\n",
    "    # we will just propagate from frame 0 to the end of the video\n",
    "    outputs_per_frame = {}\n",
    "    for response in predictor.handle_stream_request(\n",
    "        request=dict(\n",
    "            type=\"propagate_in_video\",\n",
    "            session_id=session_id,\n",
    "        )\n",
    "    ):\n",
    "        outputs_per_frame[response[\"frame_index\"]] = response[\"outputs\"]\n",
    "\n",
    "    return outputs_per_frame\n",
    "\n",
    "def abs_to_rel_coords(coords, IMG_WIDTH, IMG_HEIGHT, coord_type='point'):\n",
    "    \"\"\"Convert absolute coordinates to relative coordinates (0-1 range)\n",
    "\n",
    "    Args:\n",
    "        coords: List of coordinates\n",
    "        coord_type: 'point' for [x, y] or 'box' for [x, y, w, h]\n",
    "    \"\"\"\n",
    "    if coord_type == 'point':\n",
    "        return [[x / IMG_WIDTH, y / IMG_HEIGHT] for x, y in coords]\n",
    "    elif coord_type == 'box':\n",
    "        return [[x / IMG_WIDTH, y / IMG_HEIGHT, w / IMG_WIDTH, h / IMG_HEIGHT] for x, y, w, h in coords]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown coord_type: {coord_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3.model.sam3_video_predictor import Sam3VideoPredictorMultiGPU\n",
    "\n",
    "sam3_root = f\"/home/{os.getenv('USER')}/sam3\"\n",
    "\n",
    "# checkpoint_file = f\"{sam3_root}/assets/checkpoints/sam3_video_model_only.pt\"\n",
    "checkpoint_file = \"/checkpoint/sam3/haithamkhedr/checkpoints/sam3_dense/sam3_v2_rc2.pt\"\n",
    "has_presence_token = True\n",
    "geo_encoder_use_img_cross_attn = True\n",
    "predictor = Sam3VideoPredictorMultiGPU(\n",
    "    checkpoint_path=checkpoint_file,\n",
    "    has_presence_token=has_presence_token,\n",
    "    geo_encoder_use_img_cross_attn=geo_encoder_use_img_cross_attn,\n",
    "    gpus_to_use=gpus_to_use,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an example video\n",
    "\n",
    "We assume that the video is stored as a list of JPEG frames with filenames like `<frame_index>.jpg`.\n",
    "\n",
    "For your custom videos, you can extract their JPEG frames using ffmpeg (https://ffmpeg.org/) as follows:\n",
    "```\n",
    "ffmpeg -i <your_video>.mp4 -q:v 2 -start_number 0 <output_dir>/'%05d.jpg'\n",
    "```\n",
    "where `-q:v` generates high-quality JPEG frames and `-start_number 0` asks ffmpeg to start the JPEG file from `00000.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this video has 6 objects\n",
    "video_path = f\"{sam3_root}/assets/videos/0001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \"video_frames_for_vis\" for visualization purposes (they are not used by the model)\n",
    "if isinstance(video_path, str) and video_path.endswith(\".mp4\"):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_frames_for_vis = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames_for_vis.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "else:\n",
    "    video_frames_for_vis = glob.glob(os.path.join(video_path, \"*.jpg\"))\n",
    "    try:\n",
    "        # integer sort instead of string sort (so that e.g. \"2.jpg\" is before \"11.jpg\")\n",
    "        video_frames_for_vis.sort(key=lambda p: int(os.path.splitext(os.path.basename(p))[0]))\n",
    "    except ValueError:\n",
    "        # fallback to lexicographic sort if the format is not \"<frame_index>.jpg\"\n",
    "        print(\n",
    "            f'frame names are not in \"<frame_index>.jpg\" format: {video_frames_for_vis[:5]=}, '\n",
    "            f\"falling back to lexicographic sort.\"\n",
    "        )\n",
    "        video_frames_for_vis.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference\n",
    "### Opening an inference session on this video\n",
    "SAM 3 requires stateful inference for interactive video segmentation, so we need to initialize an **inference session** on this video.\n",
    "\n",
    "During initialization, it loads all the JPEG frames in the video directory and stores their features in the session state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"start_session\",\n",
    "        resource_path=video_path,\n",
    "    )\n",
    ")\n",
    "session_id = response[\"session_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video promptable concept segmentation with text\n",
    "\n",
    "Using SAM 3 you can describe objects using natural language,\n",
    "\n",
    "and the model will automatically detect and track all instances of that object throughout the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a text prompt on frame 0 and propagation throughout the video\n",
    "\n",
    "Note that the first call might be slower due to setting up buffers. **You can rerun all the cells below when measuring speed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: in case you already ran one text prompt and now want to switch to another text prompt\n",
    "# it's required to reset the session first (otherwise the results would be wrong)\n",
    "_ = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"reset_session\",\n",
    "        session_id=session_id,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a text prompt\n",
    "\n",
    "Here we use the text prompt \"person\" to detect all people in the video. \n",
    "\n",
    "SAM 3 will automatically identify multiple person instances and assign each a unique object ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text_str = \"person\"\n",
    "frame_idx = 0  # add a text prompt on frame 0\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\",\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        text=prompt_text_str,\n",
    "    )\n",
    ")\n",
    "out = response[\"outputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 2176083696212028,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "visualize_formatted_frame_output(\n",
    "    frame_idx,\n",
    "    video_frames_for_vis,\n",
    "    outputs_list=[prepare_masks_for_visualization({frame_idx: out})],\n",
    "    titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "    figsize=(6, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 1292061085723685,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "# now we propagate the outputs from frame 0 to the end of the video and collect all outputs\n",
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "\n",
    "# finally, we reformat the outputs for visualization and plot the outputs every 60 frames\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    print(f\"frame {frame_idx}\")\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Removing objects\n",
    "\n",
    "We can remove individual objects using their id. Let's remove object 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we pick id 2, which is the dancer in the front\n",
    "obj_id = 2\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"remove_object\",\n",
    "        session_id=session_id,\n",
    "        obj_id=obj_id,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we propagate the outputs from frame 0 to the end of the video and collect all outputs\n",
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "\n",
    "# finally, we reformat the outputs for visualization and plot the outputs every 60 frames\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    print(f\"frame {frame_idx}\")\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding new objects with point prompts\n",
    "\n",
    "In addition to text prompts, SAM 3 supports **point prompts** to add specific objects that might not be detected automatically.\n",
    "Here we add a new object by clicking on a specific location.\n",
    "\n",
    "Note: label `1` indicates a *positive click (to add a region)* while label `0` indicates a *negative click (to remove a region)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get image dimensions for coordinate conversion\n",
    "\n",
    "We compute the image dimensions once at the beginning and use them throughout for coordinate conversions.\n",
    "\n",
    "All coordinates in this example are defined in absolute pixel coordinates and then converted to relative coordinates (0-1 range) for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = Image.fromarray(load_frame(video_frames_for_vis[0]))\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = sample_img.size\n",
    "print(f\"Image dimensions: {IMG_WIDTH} x {IMG_HEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add back the dancer via point prompts.\n",
    "# we will only use positive clicks to add the dancer back.\n",
    "points_abs = [\n",
    "    [740, 450],\n",
    "    [760, 630],\n",
    "    [760, 550],\n",
    "    \n",
    "]\n",
    "# all positive clicks\n",
    "labels = np.array([1, 1, 1])\n",
    "\n",
    "frame_idx = 0\n",
    "obj_id = 2\n",
    "\n",
    "points_abs = np.array(points_abs)\n",
    "\n",
    "image = Image.fromarray(load_frame(video_frames_for_vis[0]))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_points(points_abs, labels, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()  \n",
    "\n",
    "\n",
    "# Convert to relative coordinates for the model\n",
    "points = np.array(abs_to_rel_coords(points_abs, IMG_WIDTH, IMG_HEIGHT, coord_type='point'))\n",
    "\n",
    "# convert points and labels to tensors\n",
    "points_tensor = torch.tensor(points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\",\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        points=points_tensor,\n",
    "        point_labels=points_labels_tensor,\n",
    "        obj_id=obj_id,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# now we propagate the outputs from frame 0 to the end of the video and collect all outputs\n",
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "\n",
    "# finally, we reformat the outputs for visualization and plot the outputs every 60 frames\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    print(f\"frame {frame_idx}\")\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Interactive refinement with point prompts\n",
    "\n",
    "In addition to using **point prompts** for adding objects, we can use them to refine objects.\n",
    "\n",
    "This is particularly useful when the initial segmentation from text prompts needs fine adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to only track the upper body of the dancer, so we add positive clicks to it\n",
    "# and negative clicks to the pants.\n",
    "points_abs = [\n",
    "    [740, 450],  # positive click\n",
    "    [760, 630],  # negative click\n",
    "    [760, 550],  # positive click\n",
    "]\n",
    "labels = np.array([1, 0, 1])\n",
    "\n",
    "frame_idx = 0\n",
    "refine_obj_id = 2\n",
    "\n",
    "points_abs = np.array(points_abs)\n",
    "\n",
    "image = Image.fromarray(load_frame(video_frames_for_vis[0]))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_points(points_abs, labels, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()  \n",
    "\n",
    "\n",
    "# Convert to relative coordinates for the model\n",
    "points = np.array(abs_to_rel_coords(points_abs, IMG_WIDTH, IMG_HEIGHT, coord_type='point'))\n",
    "\n",
    "# convert points and labels to tensors\n",
    "points_tensor = torch.tensor(points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\",\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        points=points_tensor,\n",
    "        point_labels=points_labels_tensor,\n",
    "        obj_id=refine_obj_id,\n",
    "    )\n",
    ")\n",
    "\n",
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    print(f\"frame {frame_idx}\")\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"reset_session\",\n",
    "        session_id=session_id,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding new objects with box prompts\n",
    "\n",
    "In addition to text and point prompts, SAM 3 supports **box prompts** to add objects.\n",
    "\n",
    "This is particularly useful if we cannot describe a concept, but we can draw a box around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding object via box only prompts\n",
    "\n",
    "In this example we prompt the model with a concrete step in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_abs = [\n",
    "    [190, 365, 89, 80]\n",
    "]\n",
    "\n",
    "boxes = abs_to_rel_coords(boxes_abs, IMG_WIDTH, IMG_HEIGHT, 'box')\n",
    "\n",
    "# we use \"visual\" as text prompt for a prompt that uses the box only\n",
    "prompt_text_str = \"visual\"\n",
    "frame_idx = 0\n",
    "\n",
    "image = Image.fromarray(load_frame(video_frames_for_vis[0]))\n",
    "image_with_box = image\n",
    "for box in boxes_abs:\n",
    "    image_with_box = draw_box_on_image(image_with_box, box, (0,255,0))\n",
    "plt.imshow(image_with_box)\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.show()\n",
    "\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\",\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        text=prompt_text_str,\n",
    "        bounding_boxes=boxes,\n",
    "        bounding_box_labels=[1] * len(boxes),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    print(f\"frame {frame_idx}\")\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"reset_session\",\n",
    "        session_id=session_id,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding object via box and text prompts\n",
    "\n",
    "In this example we prompt the model with both a text and a box prompt for the face of the person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_abs = [\n",
    "    [520, 200, 90, 40]\n",
    "]\n",
    "\n",
    "boxes = abs_to_rel_coords(boxes_abs, IMG_WIDTH, IMG_HEIGHT, 'box')\n",
    "\n",
    "prompt_text_str = \"face\"\n",
    "frame_idx = 0\n",
    "\n",
    "# visualize the concrete step box prompt\n",
    "visualize_prompt_overlay(\n",
    "    frame_idx,\n",
    "    video_frames_for_vis,\n",
    "    bounding_boxes=boxes,\n",
    "    box_labels=[1],\n",
    "    text_prompt=prompt_text_str,\n",
    ")\n",
    "\n",
    "\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\",\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        text=prompt_text_str,\n",
    "        bounding_boxes=boxes,\n",
    "        bounding_box_labels=[1] * len(boxes),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    print(f\"frame {frame_idx}\")\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refine multiple objects simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refine object 0 (face region)\n",
    "\n",
    "First, we refine the face region using negative and positive points to improve boundary precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_abs = [\n",
    "    [580, 250],  # negative point to remove unwanted area\n",
    "    [570, 220],  # positive point to keep desired area\n",
    "]\n",
    "\n",
    "point_labels = np.array([0,1])\n",
    "frame_idx = 0\n",
    "object_id = 0\n",
    "\n",
    "\n",
    "points_abs = np.array(points_abs)\n",
    "\n",
    "image = Image.fromarray(load_frame(video_frames_for_vis[0]))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_points(points_abs, point_labels, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()  \n",
    "\n",
    "\n",
    "# Convert to relative coordinates for the model\n",
    "points = np.array(abs_to_rel_coords(points_abs, IMG_WIDTH, IMG_HEIGHT, coord_type='point'))\n",
    "\n",
    "# Convert points and labels to tensors\n",
    "points_tensor = torch.tensor(points, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(point_labels, dtype=torch.int32)\n",
    "\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\",\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        points=points_tensor,\n",
    "        point_labels=labels_tensor,\n",
    "        obj_id=object_id,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    print(f\"frame {frame_idx}\")\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refine object 3 (body region)\n",
    "\n",
    "Next, we refine the body region using multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_abs = [\n",
    "    [740, 450],  # positive click\n",
    "    [770, 630],  # negative click\n",
    "    [760, 550],  # positive click\n",
    "    [760, 600],  # negative click\n",
    "\n",
    "]\n",
    "labels = np.array([1, 0, 1, 0])\n",
    "points_abs = np.array(points_abs)\n",
    "point_labels = np.array(labels)\n",
    "\n",
    "plt.close('all')\n",
    "image = Image.fromarray(load_frame(video_frames_for_vis[0]))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_points(points_abs, point_labels, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()  \n",
    "\n",
    "# Convert to relative coordinates for the model\n",
    "points = abs_to_rel_coords(points_abs,IMG_WIDTH, IMG_HEIGHT, coord_type='point')\n",
    "\n",
    "frame_idx = 0\n",
    "object_id = 2\n",
    "\n",
    "\n",
    "# Convert points and labels to tensors\n",
    "points_tensor = torch.tensor(points, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(point_labels, dtype=torch.int32)\n",
    "\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\",\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        points=points_tensor,\n",
    "        point_labels=labels_tensor,\n",
    "        obj_id=object_id,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    print(f\"frame {frame_idx}\")\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, close the inference session to free its GPU resources\n",
    "# (you may start a new session on another video)\n",
    "_ = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"close_session\",\n",
    "        session_id=session_id,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after all inference is done, we can shutdown the predictor\n",
    "# to free up the multi-GPU process group\n",
    "predictor.shutdown()"
   ]
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "0302644d-89e4-478b-b8e6-568ead852534",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
